{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Function Approximation\n",
    "\n",
    "\n",
    "Sciences Po, Spring 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Outline\n",
    "\n",
    "1. Overview of Approximation Methods\n",
    "\t1. Interpolation\n",
    "\t1. Regression\n",
    "1. Polynomial Interpolation\n",
    "1. Spline Interpolation\n",
    "1. Multidimensional Approximation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Approximation Methods\n",
    "\n",
    "* Confronted with a non-analytic function $f$ (i.e. something not like $log(x)$), we need a way to numerically represent $f$ in a computer.\n",
    "\t* If your problem is to compute a value function in a dynamic problem, you don't have an *analytic representation* of $V$.\n",
    "\t* If you need to compute an equilibrium distribution for your model, you probably can't tell it's from one parametric family or another.\n",
    "* Approximations use *data* of some kind which informs us about $f$. Most commonly, we know the function values $f(x_i)$ at a corresponding finite set of points $X = \\{x_i\\}_{i=1}^N$.\n",
    "* The task of approximation is to take that data and tell us what the function value is at $f(y),y\\not \\in X$.\n",
    "* To an economist this should sound very familiar: take a dataset, learn it's structure, and make predictions.\n",
    "* The only difference is that we can do much better here, because we have more degree's of freedom (we can choose our $X$ in $Y=\\beta X + \\epsilon$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Some Taxonomy\n",
    "\n",
    "* Local Approximations: approximate function and it's derivative $f,f'$ at a *single* point $x_0$. Taylor Series:\n",
    "\t$$ f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(x_0) + \\dots + \\frac{(x-x_0)^n}{n!}f^{n}(x_0) $$\n",
    "* Interpolation or *Colocation*: find a function $\\hat{f}$ that is a good fit to $f$, and require that $\\hat{f}$ *passes through* the points. If we think of there being a *residual* $\\epsilon_i = f(x_i) - \\hat{f}(x_i)$ at each grid point $i$, this methods succeeds in setting $\\epsilon_i=0,\\forall i$.\n",
    "* Regression: Minimize some notion of distance (or squared distance) between $\\hat{f}$ and $f$, without the requirement of pass through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doing Interpolation in Julia\n",
    "\n",
    "* In practice, you will make heavy use of high-quality interpolation packages in julia.\n",
    "* List in the end.\n",
    "* Nevertheless, function approximation is *extremely* problem-specific, so sometimes a certain approach does not work for your problem.\n",
    "* This is why we will go through the mechanics of some common methods.\n",
    "* I would like you to know where to start drilling if you need to go and hack somebody elses code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Interpolation Basics\n",
    "* Let $f$ be a smooth function mapping $\\mathbb{R}^d \\mapsto \\mathbb{R}$, and define $\\hat{f}(\\cdot;c)$ to be our parametric approximation function. We generically define this as\n",
    "\n",
    "    $$  \n",
    "        \\hat{f}(x;c) = \\sum_{j=1}^J c_j \\phi_j(x) \n",
    "    $$\n",
    "\n",
    "    where \n",
    "* $\\phi_j : \\mathbb{R}^d \\mapsto \\mathbb{R}$ is called a **basis function**,\n",
    "* $c={c_1,c_2,\\dots,c_J}$ is a coefficient vector\n",
    "* The integer $J$ is the *order* of the interpolation.\n",
    "* Our problem is to choose $(\\phi_i,c)$ in some way.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Doing Interpolation in Julia\n",
    "\n",
    "* In practice, you will make heavy use of high-quality interpolation packages in julia.\n",
    "* List in the end.\n",
    "* Nevertheless, function approximation is *extremely* problem-specific, so sometimes a certain approach does not work for your problem.\n",
    "* This is why we will go through the mechanics of some common methods.\n",
    "* I would like you to know where to start drilling if you need to go and hack somebody elses code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Interpolation Basics\n",
    "* Let $f$ be a smooth function mapping $\\mathbb{R}^d \\mapsto \\mathbb{R}$, and define $\\hat{f}(\\cdot;c)$ to be our parametric approximation function. We generically define this as\n",
    "\n",
    "    $$  \n",
    "        \\hat{f}(x;c) = \\sum_{j=1}^J c_j \\phi_j(x) \n",
    "    $$\n",
    "\n",
    "where \n",
    "\n",
    "* $\\phi_j : \\mathbb{R}^d \\mapsto \\mathbb{R}$ is called a **basis function**,\n",
    "* $c={c_1,c_2,\\dots,c_J}$ is a coefficient vector\n",
    "* The integer $J$ is the *order* of the interpolation.\n",
    "* Our problem is to choose $(\\phi_i,c)$ in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We will construct a *grid* of $M\\geq J$ points ${x_1,\\dots,x_M}$ within the domain $\\mathbb{R}^d$, and we will denote the *residuals* at each grid point by $\\epsilon = {\\epsilon_1,\\dots,\\epsilon_M}$:\n",
    "$$ \n",
    "\\left[\\begin{array}{c}\n",
    "    \\epsilon_1 \\\\\n",
    "     \\vdots \\\\\n",
    "    \\epsilon_M \\\\ \\end{array} \\right]  = \\left[\\begin{array}{c} f(x_1) \\\\ \\vdots \\\\ f(x_M)  \\end{array} \\right] - \\left[\\begin{array}{ccc} \n",
    "    \\phi_1(x_1) & \\dots & \\phi_J(x_1) \\\\   \n",
    "    \\vdots      & \\ddots & \\vdots \\\\   \n",
    "    \\phi_1(x_M) & \\dots & \\phi_J(x_M)    \n",
    "    \\end{array} \\right]  \\cdot \n",
    "    \\left[\\begin{array}{c} c_1 \\\\ \\vdots \\\\ c_J  \\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Chebyshev Polynomial and Basis Function\n",
    "\n",
    "* As before, this Basis is defined in $[-1,1]$, so for general $x\\in[a,b]$ we normalize $x$ to\n",
    "\t$$ z_i = 2\\frac{x_i-a}{b-a} -1 $$\n",
    "* There are several ways to obtain the value of the order $j$ Chebyshev polynomial at $z$, e.g. to get $\\sum_{i=0}^n\n",
    "a_i T_i(z)$.\n",
    "\t1. use definition $T_j(z) = \\cos(\\arccos(z)j)$. Or\n",
    "\t2. Recursively we have that $\\phi_j(x) = T_{j-1}(z)$, and\n",
    "\t\t$$\n",
    "        \\begin{aligned*}\n",
    "        x = y\\\\\n",
    "        x = y\n",
    "        \\end{aligned*}\n",
    "        $$\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
